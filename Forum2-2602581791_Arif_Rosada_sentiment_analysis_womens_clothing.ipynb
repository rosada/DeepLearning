{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "60fc3dd4",
      "metadata": {
        "id": "60fc3dd4"
      },
      "source": [
        "## Sentiment Analysis on Womens Clothing E-Commerce Reviews\n",
        "\n",
        "1. **Ambil Data dari Dataset**: Menggunakan Kaggle API untuk mengunduh dataset dan memuatnya ke dalam pandas DataFrame.\n",
        "2. **Konversi Rating ke Sentimen**: Mengonversi kolom rating ke kategori sentimen sesuai aturan (1,2 = negative, 3 = normal, 4,5 -positive).\n",
        "3. **Menampilkan Jumlah Data untuk Masing-Masing Kategori Sentimen** : data bernilai postive, netral dan normal\n",
        "4. **Membangun Vocab Word**: Membuat vocab dengan 10.000 kata dari dataset dan menambahkan 2 token untuk padding dan OOV.\n",
        "5. **Menampilkan 10 Kata Paling Sering Muncul**: Menampilkan 10 kata yang paling sering muncul dalam dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f263376",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "6f263376",
        "outputId": "719fb2b2-45f6-465d-88b6-f43457c7cc71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silakan unggah file .zip atau .csv dari dataset 'nicapotato/womens-ecommerce-clothing-reviews'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-df806a37-e6dd-4529-bc26-1757156d62f0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-df806a37-e6dd-4529-bc26-1757156d62f0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Womens Clothing E-Commerce Reviews.csv.zip to Womens Clothing E-Commerce Reviews.csv (1).zip\n",
            "File yang diunggah: Womens Clothing E-Commerce Reviews.csv (1).zip\n",
            "File Womens Clothing E-Commerce Reviews.csv (1).zip telah diekstrak.\n",
            "File CSV ditemukan: Womens Clothing E-Commerce Reviews.csv\n",
            "\n",
            "First few rows of the dataset:\n",
            "   Unnamed: 0  Clothing ID  Age                    Title  \\\n",
            "0           0          767   33                      NaN   \n",
            "1           1         1080   34                      NaN   \n",
            "2           2         1077   60  Some major design flaws   \n",
            "3           3         1049   50         My favorite buy!   \n",
            "4           4          847   47         Flattering shirt   \n",
            "\n",
            "                                         Review Text  Rating  Recommended IND  \\\n",
            "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
            "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
            "2  I had such high hopes for this dress and reall...       3                0   \n",
            "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
            "4  This shirt is very flattering to all due to th...       5                1   \n",
            "\n",
            "   Positive Feedback Count   Division Name Department Name Class Name  \n",
            "0                        0       Initmates        Intimate  Intimates  \n",
            "1                        4         General         Dresses    Dresses  \n",
            "2                        0         General         Dresses    Dresses  \n",
            "3                        0  General Petite         Bottoms      Pants  \n",
            "4                        6         General            Tops    Blouses  \n",
            "\n",
            "Missing Ratings: 0\n",
            "\n",
            "Jumlah Data per Kategori Sentimen:\n",
            "Sentiment\n",
            "positive    18208\n",
            "neutral      2871\n",
            "negative     2407\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Vocab Size (total kata unik): 14849\n",
            "Vocab Size yang digunakan (terbatas): 10002\n",
            "\n",
            "10 Kata yang Paling Sering Muncul:\n",
            "the: 76164\n",
            "i: 59314\n",
            "and: 49009\n",
            "a: 43017\n",
            "it: 42817\n",
            "is: 30640\n",
            "this: 25762\n",
            "to: 24577\n",
            "in: 20722\n",
            "but: 16555\n"
          ]
        }
      ],
      "source": [
        "# Import library yang diperlukan\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Langkah 1: Unggah file dataset secara manual\n",
        "print(\"Silakan unggah file .zip atau .csv dari dataset 'nicapotato/womens-ecommerce-clothing-reviews'\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Langkah 2: Cek dan proses file yang diunggah\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"File yang diunggah: {filename}\")\n",
        "\n",
        "    # Jika file adalah .zip, ekstrak isinya\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall()\n",
        "        print(f\"File {filename} telah diekstrak.\")\n",
        "        # Asumsikan file CSV bernama 'Womens Clothing E-Commerce Reviews.csv'\n",
        "        csv_file = 'Womens Clothing E-Commerce Reviews.csv'\n",
        "    else:\n",
        "        # Jika file adalah .csv, gunakan langsung\n",
        "        csv_file = filename\n",
        "\n",
        "# Langkah 3: Verifikasi keberadaan file CSV\n",
        "if os.path.exists(csv_file):\n",
        "    print(f\"File CSV ditemukan: {csv_file}\")\n",
        "else:\n",
        "    print(f\"File {csv_file} tidak ditemukan. Berikut file yang tersedia:\")\n",
        "    !ls\n",
        "    raise FileNotFoundError(f\"File {csv_file} tidak ditemukan. Pastikan nama file sesuai atau file telah diekstrak.\")\n",
        "\n",
        "# Langkah 4: Muat dataset ke pandas DataFrame\n",
        "df = pd.read_csv(csv_file)\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Langkah 5: Handle missing values\n",
        "df['Review Text'] = df['Review Text'].fillna('')  # Ganti NaN dengan string kosong\n",
        "print(f\"\\nMissing Ratings: {df['Rating'].isna().sum()}\")\n",
        "\n",
        "# Langkah 6: Konversi rating ke sentimen\n",
        "def convert_rating_to_sentiment(rating):\n",
        "    if rating in [1, 2]:\n",
        "        return 'negative'\n",
        "    elif rating == 3:\n",
        "        return 'neutral'\n",
        "    elif rating in [4, 5]:\n",
        "        return 'positive'\n",
        "\n",
        "df['Sentiment'] = df['Rating'].apply(convert_rating_to_sentiment)\n",
        "\n",
        "# Langkah 7: Tampilkan jumlah data untuk setiap kategori sentimen\n",
        "print(\"\\nJumlah Data per Kategori Sentimen:\")\n",
        "sentiment_counts = df['Sentiment'].value_counts()\n",
        "print(sentiment_counts)\n",
        "\n",
        "# Langkah 8: Bangun vocabulary dengan 10.000 kata + 2 token (<PAD>, <OOV>)\n",
        "tokenizer = Tokenizer(num_words=10000 + 2, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['Review Text'])\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Total kata unik dalam dataset\n",
        "print(f\"\\nVocab Size (total kata unik): {vocab_size}\")\n",
        "print(f\"Vocab Size yang digunakan (terbatas): 10002\")  # 10.000 kata + <PAD> + <OOV>\n",
        "\n",
        "# Langkah 9: Tampilkan 10 kata paling sering\n",
        "word_freq = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"\\n10 Kata yang Paling Sering Muncul:\")\n",
        "for word, freq in word_freq:\n",
        "    print(f\"{word}: {freq}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 1: Instal dependensi dengan versi yang kompatibel\n",
        "!pip uninstall -y numpy scipy gensim scikit-learn pandas  # Hapus versi yang ada\n",
        "!pip install numpy==1.25.2\n",
        "!pip install scipy==1.10.1\n",
        "!pip install gensim==4.3.2\n",
        "!pip install scikit-learn==1.5.2\n",
        "!pip install pandas==2.2.2\n",
        "\n",
        "# Import library yang diperlukan\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Langkah 2: Unggah file dataset secara manual\n",
        "print(\"Silakan unggah file .zip atau .csv dari dataset 'nicapotato/womens-ecommerce-clothing-reviews'\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Langkah 3: Cek dan proses file yang diunggah\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"File yang diunggah: {filename}\")\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall()\n",
        "        print(f\"File {filename} telah diekstrak.\")\n",
        "        csv_file = 'Womens Clothing E-Commerce Reviews.csv'\n",
        "    else:\n",
        "        csv_file = filename\n",
        "\n",
        "# Langkah 4: Verifikasi keberadaan file CSV\n",
        "if os.path.exists(csv_file):\n",
        "    print(f\"File CSV ditemukan: {csv_file}\")\n",
        "else:\n",
        "    print(f\"File {csv_file} tidak ditemukan. Berikut file yang tersedia:\")\n",
        "    !ls\n",
        "    raise FileNotFoundError(f\"File {csv_file} tidak ditemukan.\")\n",
        "\n",
        "# Langkah 5: Muat dataset ke pandas DataFrame\n",
        "df = pd.read_csv(csv_file)\n",
        "df['Review Text'] = df['Review Text'].fillna('')  # Handle NaN\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Langkah 6: Konversi rating ke sentimen\n",
        "def convert_rating_to_sentiment(rating):\n",
        "    if rating in [1, 2]:\n",
        "        return 'negative'\n",
        "    elif rating == 3:\n",
        "        return 'neutral'\n",
        "    elif rating in [4, 5]:\n",
        "        return 'positive'\n",
        "\n",
        "df['Sentiment'] = df['Rating'].apply(convert_rating_to_sentiment)\n",
        "print(\"\\nJumlah Data per Kategori Sentimen:\")\n",
        "print(df['Sentiment'].value_counts())\n",
        "\n",
        "# Langkah 7: Preprocessing teks\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Ubah ke lowercase\n",
        "    text = re.sub(f'[{string.punctuation}]', '', text)  # Hapus tanda baca\n",
        "    text = re.sub(r'\\d+', '', text)  # Hapus angka\n",
        "    return text\n",
        "\n",
        "df['Cleaned Review'] = df['Review Text'].apply(preprocess_text)\n",
        "\n",
        "# Langkah 8: Bagi data menjadi training (80%), validation (10%), dan testing (10%)\n",
        "X = df['Cleaned Review']\n",
        "y = df['Sentiment']\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"\\nUkuran Data:\")\n",
        "print(f\"Training: {len(X_train)} samples\")\n",
        "print(f\"Validation: {len(X_val)} samples\")\n",
        "print(f\"Testing: {len(X_test)} samples\")\n",
        "\n",
        "# Langkah 9: Tokenisasi teks untuk Word2Vec\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "train_sentences = [tokenize_text(text) for text in X_train]\n",
        "val_sentences = [tokenize_text(text) for text in X_val]\n",
        "test_sentences = [tokenize_text(text) for text in X_test]\n",
        "\n",
        "# Langkah 10: Latih model Word2Vec (CBOW dan Skip-Gram)\n",
        "# CBOW (sg=0)\n",
        "cbow_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "# Skip-Gram (sg=1)\n",
        "skipgram_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "print(\"\\nModel Word2Vec telah dilatih (CBOW dan Skip-Gram).\")\n",
        "\n",
        "# Langkah 11: Fungsi untuk membuat vektor rata-rata dari teks\n",
        "def get_average_word2vec(tokens, model, vector_size):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Vektorisasi data untuk CBOW\n",
        "X_train_cbow = np.array([get_average_word2vec(tokens, cbow_model, 100) for tokens in train_sentences])\n",
        "X_val_cbow = np.array([get_average_word2vec(tokens, cbow_model, 100) for tokens in val_sentences])\n",
        "X_test_cbow = np.array([get_average_word2vec(tokens, cbow_model, 100) for tokens in test_sentences])\n",
        "\n",
        "# Vektorisasi data untuk Skip-Gram\n",
        "X_train_skipgram = np.array([get_average_word2vec(tokens, skipgram_model, 100) for tokens in train_sentences])\n",
        "X_val_skipgram = np.array([get_average_word2vec(tokens, skipgram_model, 100) for tokens in val_sentences])\n",
        "X_test_skipgram = np.array([get_average_word2vec(tokens, skipgram_model, 100) for tokens in test_sentences])\n",
        "\n",
        "# Langkah 12: Latih dan evaluasi model klasifikasi (Logistic Regression)\n",
        "# CBOW\n",
        "clf_cbow = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_cbow.fit(X_train_cbow, y_train)\n",
        "y_pred_cbow = clf_cbow.predict(X_test_cbow)\n",
        "\n",
        "print(\"\\nPerforma Klasifikasi dengan CBOW:\")\n",
        "print(f\"Akurasi: {accuracy_score(y_test, y_pred_cbow):.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_cbow))\n",
        "\n",
        "# Skip-Gram\n",
        "clf_skipgram = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_skipgram.fit(X_train_skipgram, y_train)\n",
        "y_pred_skipgram = clf_skipgram.predict(X_test_skipgram)\n",
        "\n",
        "print(\"\\nPerforma Klasifikasi dengan Skip-Gram:\")\n",
        "print(f\"Akurasi: {accuracy_score(y_test, y_pred_skipgram):.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_skipgram))\n",
        "\n",
        "# Langkah 13: Perbandingan CBOW vs Skip-Gram\n",
        "cbow_acc = accuracy_score(y_test, y_pred_cbow)\n",
        "skipgram_acc = accuracy_score(y_test, y_pred_skipgram)\n",
        "print(\"\\nPerbandingan Performa:\")\n",
        "print(f\"Akurasi CBOW: {cbow_acc:.4f}\")\n",
        "print(f\"Akurasi Skip-Gram: {skipgram_acc:.4f}\")\n",
        "if cbow_acc > skipgram_acc:\n",
        "    print(\"CBOW lebih baik berdasarkan akurasi.\")\n",
        "elif skipgram_acc > cbow_acc:\n",
        "    print(\"Skip-Gram lebih baik berdasarkan akurasi.\")\n",
        "else:\n",
        "    print(\"CBOW dan Skip-Gram memiliki akurasi yang sama.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5jctsmifVfav",
        "outputId": "ca50c17b-5724-4e4b-be97-c6c8cb679f70"
      },
      "id": "5jctsmifVfav",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.25.2\n",
            "Uninstalling numpy-1.25.2:\n",
            "  Successfully uninstalled numpy-1.25.2\n",
            "Found existing installation: scipy 1.10.1\n",
            "Uninstalling scipy-1.10.1:\n",
            "  Successfully uninstalled scipy-1.10.1\n",
            "\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pandas as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.25.2\n",
            "  Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.28.0 requires pandas>=1.1.4, which is not installed.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "bokeh 3.7.2 requires pandas>=1.2, which is not installed.\n",
            "pytensor 2.30.3 requires scipy<2,>=1, which is not installed.\n",
            "clarabel 0.10.0 requires scipy, which is not installed.\n",
            "db-dtypes 1.4.2 requires pandas>=0.24.2, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "albumentations 2.0.6 requires scipy>=1.10.0, which is not installed.\n",
            "dask-cuda 25.2.0 requires pandas>=1.3, which is not installed.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "osqp 1.0.3 requires scipy>=0.13.2, which is not installed.\n",
            "cmdstanpy 1.2.5 requires pandas, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "xarray-einstats 0.8.0 requires scipy>=1.9, which is not installed.\n",
            "xgboost 2.1.4 requires scipy, which is not installed.\n",
            "bigframes 2.1.0 requires pandas>=1.5.3, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "arviz 0.21.0 requires pandas>=1.5.0, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "bigquery-magics 0.9.0 requires pandas>=1.1.0, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires pandas, which is not installed.\n",
            "bqplot 0.12.44 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "datascience 0.17.6 requires pandas, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "yfinance 0.2.57 requires pandas>=1.3.0, which is not installed.\n",
            "jax 0.5.2 requires scipy>=1.11.1, which is not installed.\n",
            "mizani 0.13.3 requires pandas>=2.2.0, which is not installed.\n",
            "mizani 0.13.3 requires scipy>=1.8.0, which is not installed.\n",
            "geopandas 1.0.1 requires pandas>=1.4.0, which is not installed.\n",
            "lightgbm 4.5.0 requires scipy, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "pymc 5.22.0 requires pandas>=0.24.0, which is not installed.\n",
            "pymc 5.22.0 requires scipy>=1.4.1, which is not installed.\n",
            "prophet 1.1.6 requires pandas>=1.0.4, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "umap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "panel 1.6.3 requires pandas>=1.2, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "pynndescent 0.5.13 requires scipy>=1.0, which is not installed.\n",
            "holoviews 1.20.2 requires pandas>=1.3, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "shap 0.47.2 requires pandas, which is not installed.\n",
            "shap 0.47.2 requires scikit-learn, which is not installed.\n",
            "shap 0.47.2 requires scipy, which is not installed.\n",
            "xarray 2025.3.1 requires pandas>=2.1, which is not installed.\n",
            "fastai 2.7.19 requires pandas, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.19 requires scipy, which is not installed.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "72a33347717d4e73925aa7ead96f5cc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.10.1\n",
            "  Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scipy==1.10.1) (1.25.2)\n",
            "Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "Installing collected packages: scipy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "sentence-transformers 3.4.1 requires scikit-learn, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "arviz 0.21.0 requires pandas>=1.5.0, which is not installed.\n",
            "datascience 0.17.6 requires pandas, which is not installed.\n",
            "mizani 0.13.3 requires pandas>=2.2.0, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "pymc 5.22.0 requires pandas>=0.24.0, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "shap 0.47.2 requires pandas, which is not installed.\n",
            "shap 0.47.2 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.19 requires pandas, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.10.1\n",
            "Collecting gensim==4.3.2\n",
            "  Using cached gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.2)\n",
            "Using cached gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Installing collected packages: gensim\n",
            "Successfully installed gensim-4.3.2\n",
            "Collecting scikit-learn==1.5.2\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
            "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "shap 0.47.2 requires pandas, which is not installed.\n",
            "fastai 2.7.19 requires pandas, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.5.2\n",
            "Collecting pandas==2.2.2\n",
            "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "Successfully installed pandas-2.2.2\n",
            "Silakan unggah file .zip atau .csv dari dataset 'nicapotato/womens-ecommerce-clothing-reviews'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d0f4cea6-f022-40b1-bcf8-97681480cc58\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d0f4cea6-f022-40b1-bcf8-97681480cc58\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Womens Clothing E-Commerce Reviews.csv.zip to Womens Clothing E-Commerce Reviews.csv (2).zip\n",
            "File yang diunggah: Womens Clothing E-Commerce Reviews.csv (2).zip\n",
            "File Womens Clothing E-Commerce Reviews.csv (2).zip telah diekstrak.\n",
            "File CSV ditemukan: Womens Clothing E-Commerce Reviews.csv\n",
            "\n",
            "First few rows of the dataset:\n",
            "   Unnamed: 0  Clothing ID  Age                    Title  \\\n",
            "0           0          767   33                      NaN   \n",
            "1           1         1080   34                      NaN   \n",
            "2           2         1077   60  Some major design flaws   \n",
            "3           3         1049   50         My favorite buy!   \n",
            "4           4          847   47         Flattering shirt   \n",
            "\n",
            "                                         Review Text  Rating  Recommended IND  \\\n",
            "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
            "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
            "2  I had such high hopes for this dress and reall...       3                0   \n",
            "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
            "4  This shirt is very flattering to all due to th...       5                1   \n",
            "\n",
            "   Positive Feedback Count   Division Name Department Name Class Name  \n",
            "0                        0       Initmates        Intimate  Intimates  \n",
            "1                        4         General         Dresses    Dresses  \n",
            "2                        0         General         Dresses    Dresses  \n",
            "3                        0  General Petite         Bottoms      Pants  \n",
            "4                        6         General            Tops    Blouses  \n",
            "\n",
            "Jumlah Data per Kategori Sentimen:\n",
            "Sentiment\n",
            "positive    18208\n",
            "neutral      2871\n",
            "negative     2407\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Ukuran Data:\n",
            "Training: 18788 samples\n",
            "Validation: 2349 samples\n",
            "Testing: 2349 samples\n",
            "\n",
            "Model Word2Vec telah dilatih (CBOW dan Skip-Gram).\n",
            "\n",
            "Performa Klasifikasi dengan CBOW:\n",
            "Akurasi: 0.8084\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.39      0.46       241\n",
            "     neutral       0.41      0.16      0.23       287\n",
            "    positive       0.85      0.97      0.90      1821\n",
            "\n",
            "    accuracy                           0.81      2349\n",
            "   macro avg       0.61      0.50      0.53      2349\n",
            "weighted avg       0.77      0.81      0.78      2349\n",
            "\n",
            "\n",
            "Performa Klasifikasi dengan Skip-Gram:\n",
            "Akurasi: 0.8195\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.63      0.41      0.50       241\n",
            "     neutral       0.42      0.18      0.26       287\n",
            "    positive       0.86      0.97      0.91      1821\n",
            "\n",
            "    accuracy                           0.82      2349\n",
            "   macro avg       0.64      0.52      0.56      2349\n",
            "weighted avg       0.78      0.82      0.79      2349\n",
            "\n",
            "\n",
            "Perbandingan Performa:\n",
            "Akurasi CBOW: 0.8084\n",
            "Akurasi Skip-Gram: 0.8195\n",
            "Skip-Gram lebih baik berdasarkan akurasi.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}